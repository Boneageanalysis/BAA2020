# -*- coding: utf-8 -*-
"""vggCheckpts.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14tfQ44Cp9oOl3pGpm3RPteSzktHaX7tU
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive 
drive.mount('/gdrive')
# %cd /gdrive

"""# New Section"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()0

from keras.models import Model
from keras_preprocessing.image import ImageDataGenerator
from keras.layers import Dense, Dropout, BatchNormalization
from keras.layers import Input, Conv2D, multiply, LocallyConnected2D, Lambda, Flatten, concatenate
from keras.layers import GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras import optimizers
from keras.metrics import mean_absolute_error
from keras.applications import VGG16,InceptionV3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import os
import matplotlib.pyplot as plt
import seaborn as sns

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # showing and rendering figures
# io related
#from skimage.io import imread
import os
from glob import glob
import seaborn as sns
from sklearn.model_selection import train_test_split
# not needed in Kaggle, but required in Jupyter
# %matplotlib inline

# hyperparameters
EPOCHS = 10
LEARNING_RATE = 0.006
BATCH_SIZE_TRAIN = 32
BATCH_SIZE_VAL = 256

#image parameters 
PIXELS = 128#default for Xception
CHANNELS = 3
IMG_SIZE = (PIXELS, PIXELS)
IMG_DIMS = (PIXELS, PIXELS, CHANNELS)
VALIDATION_FRACTION = 0.25
SEED = 7834

import pandas as pd
csvfile=r'/gdrive/My Drive/boneage-new.csv'
df = pd.read_csv(csvfile)

#df = pd.read_csv('C:\\Users\\darugar\\Desktop\\ml\\bone\\boneage-training-dataset.csv')
df.count

df.head(15)

files = ['/gdrive/My Drive/boneage-training-dataset/' + str(i) + '.png' for i in df['id']]
df['file'] = files
df['exists'] = df['file'].map(os.path.exists)

files = ['F:/boneage-training-dataset/boneage-training-dataset/' + str(i) + '.png' for i in df['id']]
df['file'] = files
df['exists'] = df['file'].map(os.path.exists)

df

fig, ax = plt.subplots()
ax = sns.distplot(df['boneage'], bins=10)
ax.set(xlabel='Boneage (months)', ylabel='Density',
    title='Boneage distribution');

boneage_mean = df['boneage'].mean()#mean age
boneage_div = 2*df['boneage'].std()
print(boneage_mean,"  ",boneage_div)
df['boneage_zscore'] = df['boneage'].map(lambda x:
    (x - boneage_mean) / boneage_div)
df.dropna(inplace=True)

df['gender'] = df['male'].map(lambda x: 1 if x else 0)

df['boneage_category'] = pd.cut(df['boneage'], 10)

df['gender']
df['boneage_category']
df.head()
df.shape[0]

raw_train_df, raw_valid_df = train_test_split(df, test_size=VALIDATION_FRACTION,
  random_state=2018)

raw_train_df.shape[0]
raw_valid_df.shape[0]

raw_valid_df.head()

raw_valid_df.loc[raw_valid_df['exists']==False,:]

train_df = raw_train_df.groupby(['boneage_category', 'male']).apply(
  lambda x: x.sample(500, replace=True)).reset_index(drop=True)
valid_df, test_df = train_test_split(raw_valid_df,

  test_size=VALIDATION_FRACTION, random_state=2019)

valid_df

train_df.shape[0]

test_df

fig, ax = plt.subplots()
ax = sns.distplot(train_df['boneage'], bins=10)
ax.set(xlabel='Boneage (months)', ylabel='Density',
    title='Boneage training distribution');

import keras
optim = keras.optimizers.Nadam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.0003)

#############################################dont exceute
weight_path = "{}_weights.best.hdf5".format('bone_age3')

checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1,save_best_only=True, mode='min', save_weights_only=True)

reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8,patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=5,min_lr=0.00006)
early = EarlyStopping(monitor="val_loss", mode="min", patience=8)
callbacks_list = [checkpoint, early, reduceLROnPlat]

BATCH_SIZE_TEST = len(test_df) // 3
STEP_SIZE_TEST = 3
STEP_SIZE_TRAIN = len(train_df) // BATCH_SIZE_TRAIN
STEP_SIZE_VALID = len(valid_df) // BATCH_SIZE_VAL

def gen_2inputs(imgDatGen, df, batch_size, seed, img_size):
    gen_img = imgDatGen.flow_from_dataframe(dataframe=df,
        x_col='file', y_col='boneage_zscore',
        batch_size=batch_size, seed=seed, shuffle=True, class_mode='other',
        target_size=img_size, color_mode='rgb',
        drop_duplicates=False)
    
    gen_gender = imgDatGen.flow_from_dataframe(dataframe=df,
        x_col='file', y_col='gender',
        batch_size=batch_size, seed=seed, shuffle=True, class_mode='other',
        target_size=img_size, color_mode='rgb',
        drop_duplicates=False)
    
    while True:
        X1i = gen_img.next()
        X2i = gen_gender.next()
        yield [X1i[0], X2i[1]], X1i[1]

def test_gen_2inputs(imgDatGen, df, batch_size, img_size):
    gen_img = imgDatGen.flow_from_dataframe(dataframe=df,
        x_col='file', y_col='boneage_zscore',
        batch_size=batch_size, shuffle=False, class_mode='other',
        target_size=img_size, color_mode='rgb',
        drop_duplicates=False)
    
    gen_gender = imgDatGen.flow_from_dataframe(dataframe=df,
        x_col='file', y_col='gender',
        batch_size=batch_size, shuffle=False, class_mode='other',
        target_size=img_size, color_mode='rgb',
        drop_duplicates=False)
    
    while True:
        X1i = gen_img.next()
        X2i = gen_gender.next()
        yield [X1i[0], X2i[1]], X1i[1]

train_idg = ImageDataGenerator(zoom_range=0.2,
                               fill_mode='nearest',
                               rotation_range=25,  
                               width_shift_range=0.25,  
                               height_shift_range=0.25,  
                               vertical_flip=False, 
                               horizontal_flip=True,
                               shear_range = 0.2,
                               samplewise_center=False, 
                               samplewise_std_normalization=False)

val_idg = ImageDataGenerator(width_shift_range=0.25, height_shift_range=0.25, horizontal_flip=True)

train_flow = gen_2inputs(train_idg, train_df, BATCH_SIZE_TRAIN, SEED, IMG_SIZE)

valid_flow = gen_2inputs(val_idg, valid_df, BATCH_SIZE_VAL, SEED, IMG_SIZE)

test_idg = ImageDataGenerator()

test_flow = test_gen_2inputs(test_idg, test_df, 788, IMG_SIZE)

def mae_months(in_gt, in_pred):
    return mean_absolute_error(boneage_div * in_gt, boneage_div * in_pred)

from keras import regularizers
# Two inputs. One for gender and one for images
in_layer_img = Input(shape=IMG_DIMS, name='input_img')
in_layer_gender = Input(shape=(1,), name='input_gender')
early = EarlyStopping(monitor="val_loss", mode="min",
                      patience=10)
# Pretrained neural network
base =InceptionV3(input_tensor=in_layer_img,input_shape=IMG_DIMS,  weights=None,include_top=False)

pt_depth = base.get_output_shape_at(0)[-1]
pt_features = base(in_layer_img)
bn_features = BatchNormalization()(pt_features)

# Attention layer
attn_layer = Conv2D(64, kernel_size=(1,1), padding='same', activation='relu')(bn_features)
attn_layer = Conv2D(16, kernel_size=(1,1), padding='same', activation='relu')(attn_layer)
attn_layer = LocallyConnected2D(1, kernel_size=(1,1), padding='valid',
    activation = 'sigmoid')(attn_layer)

# Applying attention to all features coming out of bn_features
up_c2_w = np.ones((1, 1, 1, pt_depth))
up_c2 = Conv2D(pt_depth, kernel_size=(1,1), padding='same',
    activation='linear', use_bias=False, weights=[up_c2_w])
up_c2.trainable = False
attn_layer = up_c2(attn_layer)

mask_features = multiply([attn_layer, bn_features])

# Global Average Pooling 2D
gap_features = GlobalAveragePooling2D()(mask_features)
gap_mask = GlobalAveragePooling2D()(attn_layer)
gap = Lambda(lambda x: x[0]/x[1], name='RescaleGAP')([gap_features, gap_mask])
gap_dr = Dropout(0.5)(gap)
dr_steps = Dropout(0.25)(Dense(1024, activation = 'elu')(gap_dr))
# This is where gender enters in the model
feature_gender = Dense(32, activation='relu')(in_layer_gender)
feature = concatenate([dr_steps, feature_gender], axis=1)
o = Dense(1000, activation='relu')(feature)
o = Dense(1000, activation='relu')(o)
o = Dense(1, activation='linear',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01))(o)
model = Model(inputs=[in_layer_img, in_layer_gender], outputs=o)
model.compile(loss='mean_absolute_error', optimizer=optim)
model.summary()

filepath='/gdrive/My Drive/inc30.h5'
checkpoints=ModelCheckpoint(filepath,monitor='loss',verbose=1,save_best_only=True,mode='min')
#callbacks_list=[checkpoints]

#checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1,save_best_only=True, mode='min', save_weights_only=True)

reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8,patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=5,min_lr=0.00006)
early = EarlyStopping(monitor="val_loss", mode="min", patience=8)
callbacks_list = [ early, reduceLROnPlat]



model_history =model.fit_generator(generator=train_flow,
                                    steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_flow,
                                    validation_steps=STEP_SIZE_VALID, epochs=30,callbacks=callbacks_list)
model.save('/gdrive/My Drive/incp30.h5')

new_model.save('/gdrive/My Drive/75+10(reg).h5')

from tensorflow.keras.models import load_model
import tensorflow as tf

new_model=load_model("/gdrive/My Drive/epoch75.h5", custom_objects = { "tf": tf })
'''
checkpoints=ModelCheckpoint(filepath,monitor='loss',verbose=1,save_best_only=True,mode='min')
#callbacks_list=[checkpoints]


#checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1,save_best_only=True, mode='min', save_weights_only=True)

reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8,patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=5,min_lr=0.00006)
early = EarlyStopping(monitor="val_loss", mode="min", patience=8)
callbacks_list = [checkpoints, early, reduceLROnPlat]
'''

new_history=new_model.fit_generator(generator=train_flow,
                                    steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_flow,
                                    validation_steps=STEP_SIZE_VALID, epochs=17)
new_model.save('/gdrive/My Drive/ep75+17.h5')

k.save('/gdrive/My Drive/ep75+7.h5')

new_model.save('/gdrive/My Drive/newmod.h5')

new_model.save('/gdrive/My Drive/epoch75.h5')

import tensorflow as tf

from tensorflow.keras.models import load_model
md=load_model("/gdrive/My Drive/ep75+14.h5", custom_objects = { "tf": tf })

#4rd time
md3=md.fit_generator(generator=train_flow,
                                    steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_flow,
                                    validation_steps=STEP_SIZE_VALID, epochs=15)
md.save('/gdrive/My Drive/fourthlr006.h5')

from tensorflow.keras.models import load_model
import tensorflow as tf
k=load_model('/gdrive/My Drive/epoch75.h5', custom_objects = { "tf": tf })

p=[7049,12447]
test=df.loc[df['id'].isin(p)]
test_flow1 = test_gen_2inputs(test_idg, test, 2, IMG_SIZE)
testx,testy=next(test_flow1)

import tensorflow as tf
model.save('/gdrive/My Drive/01epoch.h5')

#m=keras.models.load_model('modelsavedvgg.h5', custom_objects = { "tf": tf })
#k=load_model("modelsavedvgg.h5", custom_objects = { "tf": tf })

files = ['/gdrive/My Drive/boneage-test-dataset/' + str(i) + '.png' for i in df['id']]
df_input['file'] = files
df_input['exists'] = df_input['file'].map(os.path.exists)

ep=[]
loss=[]
for i in range(1,81):
  ep.append(i)
loss=[0.7060,0.6687,0.6441,0.5886,0.5387,0.5884,0.5137,0.4814,0.4584,0.4307,0.4025,0.3769,0.3560,0.3347,0.3022,0.2746,0.2573,0.2464,0.2319,0.2247,0.2113,0.2045,0.1985,0.1900,0.1856,0.1845,0.1859,0.1779,0.1672,0.1649,0.1613,0.1588,0.1519,0.1545,0.1548,0.1547,0.1478,0.1492,0.1455,0.1412,0.1424,0.1385,0.1346,0.1368,0.1323,0.1333,0.1297,0.1286,0.1258,0.1224,0.1219,0.1203,0.1189,0.1221,0.1181,0.1170,0.1142,0.1107,0.1129,0.1121,0.1096,0.1097,0.1087,0.1105,0.1072,0.1135,0.1044,0.1060,0.1049,0.1028,0.1042,0.1082,0.1024,0.1015,0.1012,0.0992,0.1012,0.0998,0.1005,0.0982]

print(len(loss))
print(len(ep))
import matplotlib.pyplot as plt
plt.plot(ep,loss,'g')
plt.title("MODEL LOSS")
plt.ylabel("LOSS")
plt.xlabel("EPOCH")
 
plt.show()



test_X,test_Y=next(test_flow)

plt.style.use("dark_background")
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = 'DejaVu Sans'
print(boneage_div)
#print(model.predict(test_X, batch_size = 263, verbose = True))
print(boneage_mean)
pred_Y = boneage_div*md.predict(test_X, batch_size = 263, verbose = True)+boneage_mean
test_Y_months = boneage_div*test_Y+boneage_mean
'''
ord_idx = np.argsort(test_Y)
ord_idx = ord_idx[np.linspace(0, len(ord_idx)-1, 4).astype(int)] # take 8 evenly spaced ones
fig, m_axs = plt.subplots(2, 2, figsize = (8, 8))
for (idx, c_ax) in zip(ord_idx, m_axs.flatten()):
    cur_img = test_X[0][idx:(idx+1)]
    c_ax.imshow(cur_img[0, :,:,0], cmap = 'bone')
    
    c_ax.set_title('Age: %2.1fY\nPredicted Age: %2.1fY' % (test_Y_months[idx]/42.0, 
                                                           pred_Y[idx]))
    c_ax.axis('off')
#fig.savefig('trained_img_predictions.png', dpi = 300)
'''

from sklearn.metrics import mean_absolute_error
mean_absolute_error(test_Y_months,pred_Y)

p=pred_Y.reshape(-1,1)
print(p.shape)
p=p.flatten()
p.shape
#print(p)
type(p)



b=test_df['id']
pth=test_df['file']
b2=test_df['boneage']

#gender1=df['male']
a=abs(p-test_Y_months)
#print(a)
df=pd.DataFrame()
df['id']=b
df['act']=test_Y_months
df['pred']=p
a.flatten()
df['diff']=a
#df['path']=pth
#df['gender1']=gender1
#df['bonage1']=test_Y_months
#df['bonage2']=b2
#df.head()
sorted=df.sort_values('diff')
df=sorted
print("best cases")
print(sorted.head(100))
print("worst cases")
print(sorted.tail(50))
#df.to_csv('/gdrive/My Drive/guime.csv')

print(df['id'],df['diff'])

df1=df[df['act']<=12]
df2=df.loc[(df['act']<=24) & (df['act']>12)]
df3=df.loc[(df['act']>24) &(df['act']<=36)]
df4=df.loc[(df['act']>36) &(df['act']<=48)]
df5=df.loc[(df['act']>48) &(df['act']<=60)]
df6=df.loc[(df['act']>60) &(df['act']<=72)]
df7=df.loc[(df['act']>72) &(df['act']<=84)]
df8=df.loc[(df['act']>84) &(df['act']<=96)]
df9=df.loc[(df['act']>96) &(df['act']<=108)]
df10=df.loc[(df['act']>108) &(df['act']<=120)]
df11=df.loc[(df['act']>120) &(df['act']<=132)]
df12=df.loc[(df['act']>132) &(df['act']<=144)]
df13=df.loc[(df['act']>144) &(df['act']<=156)]
df14=df.loc[(df['act']>156) &(df['act']<=168)]
df15=df.loc[(df['act']>168) &(df['act']<=180)]
df16=df.loc[(df['act']>180) &(df['act']<=192)]
df17=df.loc[(df['act']>192) &(df['act']<=204)]
df18=df.loc[(df['act']>204) &(df['act']<=216)]
df19=df.loc[(df['act']>216) &(df['act']<=228)]

print(mean_absolute_error(df1['act'],df1['pred']))
print(mean_absolute_error(df2['act'],df2['pred']))
print(mean_absolute_error(df3['act'],df3['pred']))
print(mean_absolute_error(df4['act'],df4['pred']))
print(mean_absolute_error(df5['act'],df5['pred']))
print(mean_absolute_error(df6['act'],df6['pred']))
print(mean_absolute_error(df7['act'],df7['pred']))
print(mean_absolute_error(df8['act'],df8['pred']))
print(mean_absolute_error(df9['act'],df9['pred']))
print(mean_absolute_error(df10['act'],df10['pred']))
print(mean_absolute_error(df11['act'],df11['pred']))
print(mean_absolute_error(df12['act'],df12['pred']))
print(mean_absolute_error(df13['act'],df13['pred']))
print(mean_absolute_error(df14['act'],df14['pred']))
print(mean_absolute_error(df15['act'],df15['pred']))
print(mean_absolute_error(df16['act'],df16['pred']))
print(mean_absolute_error(df17['act'],df17['pred']))
print(mean_absolute_error(df18['act'],df18['pred']))
print(mean_absolute_error(df19['act'],df19['pred']))

import pandas as pd
act=pd.DataFrame(test_Y_months)
p=pd.DataFrame(pred_Y)
diff=pd.DataFrame(abs(act-p))

print(diff)

from sklearn.metrics import mean_absolute_error
mean_absolute_error(test_Y_months,pred_Y)

diff.describe()

np.sqrt(((test_Y_months-pred_Y)**2).mean())

from sklearn.metrics import mean_absolute_error
mean_absolute_error(test_Y_months,pred_Y)

from scipy import stats
t2,p2=stats.ttest_ind(test_Y_months,pred_Y)
print("t="+str(t2))
print("p="+str(p2))

